{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prabhakar/.local/lib/python3.5/site-packages/IPython/core/magics/pylab.py:160: UserWarning: pylab import has clobbered these variables: ['imread']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.misc import imread\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 128\n",
    "rng = np.random.RandomState(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_dir = os.path.abspath('..')\n",
    "data_dir = os.path.join(root_dir, 'data')\n",
    "\n",
    "# check for existence\n",
    "os.path.exists(root_dir)\n",
    "os.path.exists(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(os.path.join(data_dir, 'train.csv'), dtype=np.float32)\n",
    "test = pd.read_csv(os.path.join(data_dir, 'test.csv'), header=None, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train.iloc[:, :-1].values\n",
    "test_x = test.iloc[:, :].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_size = int(train_x.shape[0]*0.7)\n",
    "\n",
    "train_x, val_x = train_x[:split_size], train_x[split_size:]\n",
    "train_y, val_y = train.popularity.values[:split_size], train.popularity.values[split_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auxilary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_to_one_hot(labels_dense, num_classes=4):\n",
    "    \"\"\"Convert class labels from scalars to one-hot vectors\"\"\"\n",
    "    s = pd.Series(list(range(1,num_classes+1,1)))\n",
    "    labels_one_hot = pd.get_dummies(labels_dense)\n",
    "    \n",
    "#     num_labels = labels_dense.shape[0]\n",
    "#     index_offset = np.arange(num_labels) * num_classes\n",
    "#     labels_one_hot = np.zeros((num_labels, num_classes))\n",
    "#     labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    "    \n",
    "    return labels_one_hot\n",
    "\n",
    "def preproc(unclean_batch_x):\n",
    "    \"\"\"Convert values to range 0-1, with mean centered\"\"\"\n",
    "    temp_batch = (unclean_batch_x-unclean_batch_x.min()) / (unclean_batch_x.max()-unclean_batch_x.min())\n",
    "    \n",
    "    return temp_batch\n",
    "\n",
    "def batch_creator(batch_size, dataset_length, dataset_name):\n",
    "    \"\"\"Create batch with random samples and return appropriate format\"\"\"\n",
    "    batch_mask = rng.choice(dataset_length, batch_size)\n",
    "    \n",
    "    batch_x = eval(dataset_name + '_x')[[batch_mask]].reshape(-1, input_num_units)\n",
    "    batch_x = preproc(batch_x)\n",
    "    \n",
    "    if dataset_name == 'train':\n",
    "        batch_y = eval(dataset_name).ix[batch_mask, 'popularity'].values\n",
    "        batch_y = dense_to_one_hot(batch_y)\n",
    "        \n",
    "    return batch_x, batch_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "### set all variables\n",
    "\n",
    "# number of neurons in each layer\n",
    "input_num_units = 6\n",
    "hidden_num_units = 650\n",
    "output_num_units = 4\n",
    "\n",
    "# define placeholders\n",
    "x = tf.placeholder(tf.float32, [None, input_num_units])\n",
    "y = tf.placeholder(tf.float32, [None, output_num_units])\n",
    "\n",
    "# set remaining variables\n",
    "epochs = 498\n",
    "batch_size = 512\n",
    "learning_rate = 0.005\n",
    "\n",
    "### define weights and biases of the neural network (refer this article if you don't understand the terminologies)\n",
    "\n",
    "weights = {\n",
    "    'hidden': tf.Variable(tf.random_normal([input_num_units, hidden_num_units], seed=seed)),\n",
    "    'output': tf.Variable(tf.random_normal([hidden_num_units, output_num_units], seed=seed))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'hidden': tf.Variable(tf.random_normal([hidden_num_units], seed=seed)),\n",
    "    'output': tf.Variable(tf.random_normal([output_num_units], seed=seed))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computational Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer = tf.add(tf.matmul(x, weights['hidden']), biases['hidden'])\n",
    "hidden_layer = tf.nn.relu(hidden_layer)\n",
    "\n",
    "output_layer = tf.matmul(hidden_layer, weights['output']) + biases['output']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=output_layer))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:27: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 cost = 3.88131\n",
      "Epoch: 2 cost = 2.88053\n",
      "Epoch: 3 cost = 2.82461\n",
      "Epoch: 4 cost = 2.58580\n",
      "Epoch: 5 cost = 2.33191\n",
      "Epoch: 6 cost = 2.44481\n",
      "Epoch: 7 cost = 2.08236\n",
      "Epoch: 8 cost = 1.96777\n",
      "Epoch: 9 cost = 1.74780\n",
      "Epoch: 10 cost = 1.82522\n",
      "Epoch: 11 cost = 1.74649\n",
      "Epoch: 12 cost = 1.80884\n",
      "Epoch: 13 cost = 1.59858\n",
      "Epoch: 14 cost = 1.46942\n",
      "Epoch: 15 cost = 1.41847\n",
      "Epoch: 16 cost = 1.34339\n",
      "Epoch: 17 cost = 1.19429\n",
      "Epoch: 18 cost = 1.22432\n",
      "Epoch: 19 cost = 1.17421\n",
      "Epoch: 20 cost = 1.10748\n",
      "Epoch: 21 cost = 1.00562\n",
      "Epoch: 22 cost = 0.93378\n",
      "Epoch: 23 cost = 1.09359\n",
      "Epoch: 24 cost = 1.01978\n",
      "Epoch: 25 cost = 0.95891\n",
      "Epoch: 26 cost = 0.89217\n",
      "Epoch: 27 cost = 0.90632\n",
      "Epoch: 28 cost = 0.89167\n",
      "Epoch: 29 cost = 0.85212\n",
      "Epoch: 30 cost = 0.89943\n",
      "Epoch: 31 cost = 0.81073\n",
      "Epoch: 32 cost = 0.80039\n",
      "Epoch: 33 cost = 0.82612\n",
      "Epoch: 34 cost = 0.76147\n",
      "Epoch: 35 cost = 0.75358\n",
      "Epoch: 36 cost = 0.73932\n",
      "Epoch: 37 cost = 0.71289\n",
      "Epoch: 38 cost = 0.78354\n",
      "Epoch: 39 cost = 0.71961\n",
      "Epoch: 40 cost = 0.76190\n",
      "Epoch: 41 cost = 0.65051\n",
      "Epoch: 42 cost = 0.61133\n",
      "Epoch: 43 cost = 0.62435\n",
      "Epoch: 44 cost = 0.70150\n",
      "Epoch: 45 cost = 0.61748\n",
      "Epoch: 46 cost = 0.70222\n",
      "Epoch: 47 cost = 0.67158\n",
      "Epoch: 48 cost = 0.71926\n",
      "Epoch: 49 cost = 0.56737\n",
      "Epoch: 50 cost = 0.66570\n",
      "Epoch: 51 cost = 0.57840\n",
      "Epoch: 52 cost = 0.55525\n",
      "Epoch: 53 cost = 0.60542\n",
      "Epoch: 54 cost = 0.63319\n",
      "Epoch: 55 cost = 0.57143\n",
      "Epoch: 56 cost = 0.54964\n",
      "Epoch: 57 cost = 0.59240\n",
      "Epoch: 58 cost = 0.56877\n",
      "Epoch: 59 cost = 0.61516\n",
      "Epoch: 60 cost = 0.56848\n",
      "Epoch: 61 cost = 0.54863\n",
      "Epoch: 62 cost = 0.54765\n",
      "Epoch: 63 cost = 0.51046\n",
      "Epoch: 64 cost = 0.59344\n",
      "Epoch: 65 cost = 0.59838\n",
      "Epoch: 66 cost = 0.58312\n",
      "Epoch: 67 cost = 0.50721\n",
      "Epoch: 68 cost = 0.56455\n",
      "Epoch: 69 cost = 0.49576\n",
      "Epoch: 70 cost = 0.49983\n",
      "Epoch: 71 cost = 0.50436\n",
      "Epoch: 72 cost = 0.51195\n",
      "Epoch: 73 cost = 0.53219\n",
      "Epoch: 74 cost = 0.50681\n",
      "Epoch: 75 cost = 0.54423\n",
      "Epoch: 76 cost = 0.52530\n",
      "Epoch: 77 cost = 0.48587\n",
      "Epoch: 78 cost = 0.55508\n",
      "Epoch: 79 cost = 0.50297\n",
      "Epoch: 80 cost = 0.49661\n",
      "Epoch: 81 cost = 0.51683\n",
      "Epoch: 82 cost = 0.49800\n",
      "Epoch: 83 cost = 0.44657\n",
      "Epoch: 84 cost = 0.44165\n",
      "Epoch: 85 cost = 0.51513\n",
      "Epoch: 86 cost = 0.49505\n",
      "Epoch: 87 cost = 0.50139\n",
      "Epoch: 88 cost = 0.54630\n",
      "Epoch: 89 cost = 0.48619\n",
      "Epoch: 90 cost = 0.43508\n",
      "Epoch: 91 cost = 0.45006\n",
      "Epoch: 92 cost = 0.40802\n",
      "Epoch: 93 cost = 0.45973\n",
      "Epoch: 94 cost = 0.50656\n",
      "Epoch: 95 cost = 0.49097\n",
      "Epoch: 96 cost = 0.41751\n",
      "Epoch: 97 cost = 0.47878\n",
      "Epoch: 98 cost = 0.44321\n",
      "Epoch: 99 cost = 0.46407\n",
      "Epoch: 100 cost = 0.40907\n",
      "Epoch: 101 cost = 0.44566\n",
      "Epoch: 102 cost = 0.45119\n",
      "Epoch: 103 cost = 0.42438\n",
      "Epoch: 104 cost = 0.40206\n",
      "Epoch: 105 cost = 0.45221\n",
      "Epoch: 106 cost = 0.40615\n",
      "Epoch: 107 cost = 0.44653\n",
      "Epoch: 108 cost = 0.41269\n",
      "Epoch: 109 cost = 0.40796\n",
      "Epoch: 110 cost = 0.43444\n",
      "Epoch: 111 cost = 0.41887\n",
      "Epoch: 112 cost = 0.43937\n",
      "Epoch: 113 cost = 0.41843\n",
      "Epoch: 114 cost = 0.40937\n",
      "Epoch: 115 cost = 0.50700\n",
      "Epoch: 116 cost = 0.43873\n",
      "Epoch: 117 cost = 0.40385\n",
      "Epoch: 118 cost = 0.44897\n",
      "Epoch: 119 cost = 0.39818\n",
      "Epoch: 120 cost = 0.40451\n",
      "Epoch: 121 cost = 0.42179\n",
      "Epoch: 122 cost = 0.38839\n",
      "Epoch: 123 cost = 0.36493\n",
      "Epoch: 124 cost = 0.35925\n",
      "Epoch: 125 cost = 0.39671\n",
      "Epoch: 126 cost = 0.36485\n",
      "Epoch: 127 cost = 0.36565\n",
      "Epoch: 128 cost = 0.37772\n",
      "Epoch: 129 cost = 0.38443\n",
      "Epoch: 130 cost = 0.38407\n",
      "Epoch: 131 cost = 0.36332\n",
      "Epoch: 132 cost = 0.37415\n",
      "Epoch: 133 cost = 0.36260\n",
      "Epoch: 134 cost = 0.36816\n",
      "Epoch: 135 cost = 0.37043\n",
      "Epoch: 136 cost = 0.35183\n",
      "Epoch: 137 cost = 0.37608\n",
      "Epoch: 138 cost = 0.36960\n",
      "Epoch: 139 cost = 0.37363\n",
      "Epoch: 140 cost = 0.37629\n",
      "Epoch: 141 cost = 0.36378\n",
      "Epoch: 142 cost = 0.38462\n",
      "Epoch: 143 cost = 0.40923\n",
      "Epoch: 144 cost = 0.36692\n",
      "Epoch: 145 cost = 0.37069\n",
      "Epoch: 146 cost = 0.37471\n",
      "Epoch: 147 cost = 0.34035\n",
      "Epoch: 148 cost = 0.35595\n",
      "Epoch: 149 cost = 0.37098\n",
      "Epoch: 150 cost = 0.32608\n",
      "Epoch: 151 cost = 0.34750\n",
      "Epoch: 152 cost = 0.39919\n",
      "Epoch: 153 cost = 0.37265\n",
      "Epoch: 154 cost = 0.37535\n",
      "Epoch: 155 cost = 0.40935\n",
      "Epoch: 156 cost = 0.35724\n",
      "Epoch: 157 cost = 0.35299\n",
      "Epoch: 158 cost = 0.39029\n",
      "Epoch: 159 cost = 0.33719\n",
      "Epoch: 160 cost = 0.31562\n",
      "Epoch: 161 cost = 0.37314\n",
      "Epoch: 162 cost = 0.31724\n",
      "Epoch: 163 cost = 0.33535\n",
      "Epoch: 164 cost = 0.35033\n",
      "Epoch: 165 cost = 0.36988\n",
      "Epoch: 166 cost = 0.32617\n",
      "Epoch: 167 cost = 0.35509\n",
      "Epoch: 168 cost = 0.31538\n",
      "Epoch: 169 cost = 0.33428\n",
      "Epoch: 170 cost = 0.31824\n",
      "Epoch: 171 cost = 0.35768\n",
      "Epoch: 172 cost = 0.32737\n",
      "Epoch: 173 cost = 0.35061\n",
      "Epoch: 174 cost = 0.31586\n",
      "Epoch: 175 cost = 0.32118\n",
      "Epoch: 176 cost = 0.33956\n",
      "Epoch: 177 cost = 0.31408\n",
      "Epoch: 178 cost = 0.30954\n",
      "Epoch: 179 cost = 0.35327\n",
      "Epoch: 180 cost = 0.30870\n",
      "Epoch: 181 cost = 0.33625\n",
      "Epoch: 182 cost = 0.30236\n",
      "Epoch: 183 cost = 0.33440\n",
      "Epoch: 184 cost = 0.33117\n",
      "Epoch: 185 cost = 0.33602\n",
      "Epoch: 186 cost = 0.35649\n",
      "Epoch: 187 cost = 0.33383\n",
      "Epoch: 188 cost = 0.33574\n",
      "Epoch: 189 cost = 0.28713\n",
      "Epoch: 190 cost = 0.34829\n",
      "Epoch: 191 cost = 0.31781\n",
      "Epoch: 192 cost = 0.34786\n",
      "Epoch: 193 cost = 0.34461\n",
      "Epoch: 194 cost = 0.28944\n",
      "Epoch: 195 cost = 0.28855\n",
      "Epoch: 196 cost = 0.32444\n",
      "Epoch: 197 cost = 0.31930\n",
      "Epoch: 198 cost = 0.33055\n",
      "Epoch: 199 cost = 0.27414\n",
      "Epoch: 200 cost = 0.31935\n",
      "Epoch: 201 cost = 0.34999\n",
      "Epoch: 202 cost = 0.31026\n",
      "Epoch: 203 cost = 0.30515\n",
      "Epoch: 204 cost = 0.29478\n",
      "Epoch: 205 cost = 0.29586\n",
      "Epoch: 206 cost = 0.35681\n",
      "Epoch: 207 cost = 0.28555\n",
      "Epoch: 208 cost = 0.31383\n",
      "Epoch: 209 cost = 0.32548\n",
      "Epoch: 210 cost = 0.30461\n",
      "Epoch: 211 cost = 0.32887\n",
      "Epoch: 212 cost = 0.33332\n",
      "Epoch: 213 cost = 0.31651\n",
      "Epoch: 214 cost = 0.30489\n",
      "Epoch: 215 cost = 0.31843\n",
      "Epoch: 216 cost = 0.27270\n",
      "Epoch: 217 cost = 0.28998\n",
      "Epoch: 218 cost = 0.27844\n",
      "Epoch: 219 cost = 0.30188\n",
      "Epoch: 220 cost = 0.27492\n",
      "Epoch: 221 cost = 0.33561\n",
      "Epoch: 222 cost = 0.28952\n",
      "Epoch: 223 cost = 0.35325\n",
      "Epoch: 224 cost = 0.30396\n",
      "Epoch: 225 cost = 0.31837\n",
      "Epoch: 226 cost = 0.28212\n",
      "Epoch: 227 cost = 0.26808\n",
      "Epoch: 228 cost = 0.26024\n",
      "Epoch: 229 cost = 0.29493\n",
      "Epoch: 230 cost = 0.29494\n",
      "Epoch: 231 cost = 0.25790\n",
      "Epoch: 232 cost = 0.29841\n",
      "Epoch: 233 cost = 0.30309\n",
      "Epoch: 234 cost = 0.30675\n",
      "Epoch: 235 cost = 0.28676\n",
      "Epoch: 236 cost = 0.29649\n",
      "Epoch: 237 cost = 0.31980\n",
      "Epoch: 238 cost = 0.25577\n",
      "Epoch: 239 cost = 0.29257\n",
      "Epoch: 240 cost = 0.30441\n",
      "Epoch: 241 cost = 0.30512\n",
      "Epoch: 242 cost = 0.28303\n",
      "Epoch: 243 cost = 0.30285\n",
      "Epoch: 244 cost = 0.29037\n",
      "Epoch: 245 cost = 0.29006\n",
      "Epoch: 246 cost = 0.28465\n",
      "Epoch: 247 cost = 0.26930\n",
      "Epoch: 248 cost = 0.27017\n",
      "Epoch: 249 cost = 0.31243\n",
      "Epoch: 250 cost = 0.28362\n",
      "Epoch: 251 cost = 0.27300\n",
      "Epoch: 252 cost = 0.29324\n",
      "Epoch: 253 cost = 0.26259\n",
      "Epoch: 254 cost = 0.31236\n",
      "Epoch: 255 cost = 0.27527\n",
      "Epoch: 256 cost = 0.29796\n",
      "Epoch: 257 cost = 0.28273\n",
      "Epoch: 258 cost = 0.24981\n",
      "Epoch: 259 cost = 0.28768\n",
      "Epoch: 260 cost = 0.28825\n",
      "Epoch: 261 cost = 0.25483\n",
      "Epoch: 262 cost = 0.28500\n",
      "Epoch: 263 cost = 0.28874\n",
      "Epoch: 264 cost = 0.27035\n",
      "Epoch: 265 cost = 0.27344\n",
      "Epoch: 266 cost = 0.29806\n",
      "Epoch: 267 cost = 0.30102\n",
      "Epoch: 268 cost = 0.26785\n",
      "Epoch: 269 cost = 0.29989\n",
      "Epoch: 270 cost = 0.24518\n",
      "Epoch: 271 cost = 0.27656\n",
      "Epoch: 272 cost = 0.29473\n",
      "Epoch: 273 cost = 0.26564\n",
      "Epoch: 274 cost = 0.27046\n",
      "Epoch: 275 cost = 0.27120\n",
      "Epoch: 276 cost = 0.26639\n",
      "Epoch: 277 cost = 0.26252\n",
      "Epoch: 278 cost = 0.26304\n",
      "Epoch: 279 cost = 0.27699\n",
      "Epoch: 280 cost = 0.27673\n",
      "Epoch: 281 cost = 0.24701\n",
      "Epoch: 282 cost = 0.27232\n",
      "Epoch: 283 cost = 0.26528\n",
      "Epoch: 284 cost = 0.26530\n",
      "Epoch: 285 cost = 0.28475\n",
      "Epoch: 286 cost = 0.26563\n",
      "Epoch: 287 cost = 0.27199\n",
      "Epoch: 288 cost = 0.25100\n",
      "Epoch: 289 cost = 0.27566\n",
      "Epoch: 290 cost = 0.28163\n",
      "Epoch: 291 cost = 0.24351\n",
      "Epoch: 292 cost = 0.21707\n",
      "Epoch: 293 cost = 0.23579\n",
      "Epoch: 294 cost = 0.27271\n",
      "Epoch: 295 cost = 0.25241\n",
      "Epoch: 296 cost = 0.25000\n",
      "Epoch: 297 cost = 0.26905\n",
      "Epoch: 298 cost = 0.31554\n",
      "Epoch: 299 cost = 0.30140\n",
      "Epoch: 300 cost = 0.27060\n",
      "Epoch: 301 cost = 0.23248\n",
      "Epoch: 302 cost = 0.24776\n",
      "Epoch: 303 cost = 0.25851\n",
      "Epoch: 304 cost = 0.25592\n",
      "Epoch: 305 cost = 0.25932\n",
      "Epoch: 306 cost = 0.25882\n",
      "Epoch: 307 cost = 0.24134\n",
      "Epoch: 308 cost = 0.24478\n",
      "Epoch: 309 cost = 0.27085\n",
      "Epoch: 310 cost = 0.26804\n",
      "Epoch: 311 cost = 0.29052\n",
      "Epoch: 312 cost = 0.25024\n",
      "Epoch: 313 cost = 0.26515\n",
      "Epoch: 314 cost = 0.25390\n",
      "Epoch: 315 cost = 0.26105\n",
      "Epoch: 316 cost = 0.22964\n",
      "Epoch: 317 cost = 0.24345\n",
      "Epoch: 318 cost = 0.23221\n",
      "Epoch: 319 cost = 0.27388\n",
      "Epoch: 320 cost = 0.27184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 321 cost = 0.29029\n",
      "Epoch: 322 cost = 0.25700\n",
      "Epoch: 323 cost = 0.27404\n",
      "Epoch: 324 cost = 0.26867\n",
      "Epoch: 325 cost = 0.25165\n",
      "Epoch: 326 cost = 0.23586\n",
      "Epoch: 327 cost = 0.25198\n",
      "Epoch: 328 cost = 0.23353\n",
      "Epoch: 329 cost = 0.24476\n",
      "Epoch: 330 cost = 0.24982\n",
      "Epoch: 331 cost = 0.23253\n",
      "Epoch: 332 cost = 0.27382\n",
      "Epoch: 333 cost = 0.23291\n",
      "Epoch: 334 cost = 0.23271\n",
      "Epoch: 335 cost = 0.26035\n",
      "Epoch: 336 cost = 0.24028\n",
      "Epoch: 337 cost = 0.24947\n",
      "Epoch: 338 cost = 0.23193\n",
      "Epoch: 339 cost = 0.23930\n",
      "Epoch: 340 cost = 0.24416\n",
      "Epoch: 341 cost = 0.25305\n",
      "Epoch: 342 cost = 0.22932\n",
      "Epoch: 343 cost = 0.23547\n",
      "Epoch: 344 cost = 0.26015\n",
      "Epoch: 345 cost = 0.27109\n",
      "Epoch: 346 cost = 0.23074\n",
      "Epoch: 347 cost = 0.24350\n",
      "Epoch: 348 cost = 0.24099\n",
      "Epoch: 349 cost = 0.26988\n",
      "Epoch: 350 cost = 0.23341\n",
      "Epoch: 351 cost = 0.22866\n",
      "Epoch: 352 cost = 0.26907\n",
      "Epoch: 353 cost = 0.22411\n",
      "Epoch: 354 cost = 0.22578\n",
      "Epoch: 355 cost = 0.22651\n",
      "Epoch: 356 cost = 0.22035\n",
      "Epoch: 357 cost = 0.24969\n",
      "Epoch: 358 cost = 0.26522\n",
      "Epoch: 359 cost = 0.23428\n",
      "Epoch: 360 cost = 0.21559\n",
      "Epoch: 361 cost = 0.24162\n",
      "Epoch: 362 cost = 0.24321\n",
      "Epoch: 363 cost = 0.26083\n",
      "Epoch: 364 cost = 0.26181\n",
      "Epoch: 365 cost = 0.24841\n",
      "Epoch: 366 cost = 0.23695\n",
      "Epoch: 367 cost = 0.24132\n",
      "Epoch: 368 cost = 0.23055\n",
      "Epoch: 369 cost = 0.23144\n",
      "Epoch: 370 cost = 0.21702\n",
      "Epoch: 371 cost = 0.24735\n",
      "Epoch: 372 cost = 0.23509\n",
      "Epoch: 373 cost = 0.22798\n",
      "Epoch: 374 cost = 0.21230\n",
      "Epoch: 375 cost = 0.24289\n",
      "Epoch: 376 cost = 0.21408\n",
      "Epoch: 377 cost = 0.25509\n",
      "Epoch: 378 cost = 0.22869\n",
      "Epoch: 379 cost = 0.24674\n",
      "Epoch: 380 cost = 0.25833\n",
      "Epoch: 381 cost = 0.20747\n",
      "Epoch: 382 cost = 0.25857\n",
      "Epoch: 383 cost = 0.22061\n",
      "Epoch: 384 cost = 0.25004\n",
      "Epoch: 385 cost = 0.25016\n",
      "Epoch: 386 cost = 0.25843\n",
      "Epoch: 387 cost = 0.23400\n",
      "Epoch: 388 cost = 0.21434\n",
      "Epoch: 389 cost = 0.20844\n",
      "Epoch: 390 cost = 0.24262\n",
      "Epoch: 391 cost = 0.23732\n",
      "Epoch: 392 cost = 0.23272\n",
      "Epoch: 393 cost = 0.20709\n",
      "Epoch: 394 cost = 0.25592\n",
      "Epoch: 395 cost = 0.22433\n",
      "Epoch: 396 cost = 0.23899\n",
      "Epoch: 397 cost = 0.23812\n",
      "Epoch: 398 cost = 0.25439\n",
      "Epoch: 399 cost = 0.21376\n",
      "Epoch: 400 cost = 0.22410\n",
      "Epoch: 401 cost = 0.24811\n",
      "Epoch: 402 cost = 0.20874\n",
      "Epoch: 403 cost = 0.23702\n",
      "Epoch: 404 cost = 0.26662\n",
      "Epoch: 405 cost = 0.24835\n",
      "Epoch: 406 cost = 0.24359\n",
      "Epoch: 407 cost = 0.21471\n",
      "Epoch: 408 cost = 0.21244\n",
      "Epoch: 409 cost = 0.22412\n",
      "Epoch: 410 cost = 0.20932\n",
      "Epoch: 411 cost = 0.20966\n",
      "Epoch: 412 cost = 0.23360\n",
      "Epoch: 413 cost = 0.22151\n",
      "Epoch: 414 cost = 0.19424\n",
      "Epoch: 415 cost = 0.22050\n",
      "Epoch: 416 cost = 0.26381\n",
      "Epoch: 417 cost = 0.21134\n",
      "Epoch: 418 cost = 0.24133\n",
      "Epoch: 419 cost = 0.21522\n",
      "Epoch: 420 cost = 0.20483\n",
      "Epoch: 421 cost = 0.21017\n",
      "Epoch: 422 cost = 0.24086\n",
      "Epoch: 423 cost = 0.20745\n",
      "Epoch: 424 cost = 0.21423\n",
      "Epoch: 425 cost = 0.22258\n",
      "Epoch: 426 cost = 0.22522\n",
      "Epoch: 427 cost = 0.20665\n",
      "Epoch: 428 cost = 0.21413\n",
      "Epoch: 429 cost = 0.23604\n",
      "Epoch: 430 cost = 0.23964\n",
      "Epoch: 431 cost = 0.22982\n",
      "Epoch: 432 cost = 0.21878\n",
      "Epoch: 433 cost = 0.22753\n",
      "Epoch: 434 cost = 0.22650\n",
      "Epoch: 435 cost = 0.21650\n",
      "Epoch: 436 cost = 0.20711\n",
      "Epoch: 437 cost = 0.22205\n",
      "Epoch: 438 cost = 0.21993\n",
      "Epoch: 439 cost = 0.22999\n",
      "Epoch: 440 cost = 0.20585\n",
      "Epoch: 441 cost = 0.25149\n",
      "Epoch: 442 cost = 0.23060\n",
      "Epoch: 443 cost = 0.25195\n",
      "Epoch: 444 cost = 0.22947\n",
      "Epoch: 445 cost = 0.21264\n",
      "Epoch: 446 cost = 0.23709\n",
      "Epoch: 447 cost = 0.19573\n",
      "Epoch: 448 cost = 0.19871\n",
      "Epoch: 449 cost = 0.24156\n",
      "Epoch: 450 cost = 0.23205\n",
      "Epoch: 451 cost = 0.20480\n",
      "Epoch: 452 cost = 0.21365\n",
      "Epoch: 453 cost = 0.18836\n",
      "Epoch: 454 cost = 0.24977\n",
      "Epoch: 455 cost = 0.22140\n",
      "Epoch: 456 cost = 0.19085\n",
      "Epoch: 457 cost = 0.22226\n",
      "Epoch: 458 cost = 0.20081\n",
      "Epoch: 459 cost = 0.21792\n",
      "Epoch: 460 cost = 0.20807\n",
      "Epoch: 461 cost = 0.24096\n",
      "Epoch: 462 cost = 0.23169\n",
      "Epoch: 463 cost = 0.22190\n",
      "Epoch: 464 cost = 0.22518\n",
      "Epoch: 465 cost = 0.21970\n",
      "Epoch: 466 cost = 0.20917\n",
      "Epoch: 467 cost = 0.21308\n",
      "Epoch: 468 cost = 0.21702\n",
      "Epoch: 469 cost = 0.21739\n",
      "Epoch: 470 cost = 0.24538\n",
      "Epoch: 471 cost = 0.22412\n",
      "Epoch: 472 cost = 0.20263\n",
      "Epoch: 473 cost = 0.24752\n",
      "Epoch: 474 cost = 0.21462\n",
      "Epoch: 475 cost = 0.19563\n",
      "Epoch: 476 cost = 0.22057\n",
      "Epoch: 477 cost = 0.22046\n",
      "Epoch: 478 cost = 0.18989\n",
      "Epoch: 479 cost = 0.21852\n",
      "Epoch: 480 cost = 0.22663\n",
      "Epoch: 481 cost = 0.23065\n",
      "Epoch: 482 cost = 0.20790\n",
      "Epoch: 483 cost = 0.18741\n",
      "Epoch: 484 cost = 0.19600\n",
      "Epoch: 485 cost = 0.24194\n",
      "Epoch: 486 cost = 0.20364\n",
      "Epoch: 487 cost = 0.22710\n",
      "Epoch: 488 cost = 0.22074\n",
      "Epoch: 489 cost = 0.19485\n",
      "Epoch: 490 cost = 0.22548\n",
      "Epoch: 491 cost = 0.17246\n",
      "Epoch: 492 cost = 0.23581\n",
      "Epoch: 493 cost = 0.22730\n",
      "Epoch: 494 cost = 0.20829\n",
      "Epoch: 495 cost = 0.19922\n",
      "Epoch: 496 cost = 0.23823\n",
      "Epoch: 497 cost = 0.21829\n",
      "Epoch: 498 cost = 0.22143\n",
      "\n",
      "Training complete!\n",
      "Validation Accuracy: 0.12883435\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        avg_cost = 0\n",
    "        total_batch = int(train.shape[0]/batch_size)\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = batch_creator(batch_size, train_x.shape[0], 'train')\n",
    "            _, c = sess.run([optimizer, cost], feed_dict = {x: batch_x, y: batch_y})\n",
    "            \n",
    "            avg_cost += c / total_batch\n",
    "            \n",
    "        print( \"Epoch:\", (epoch+1), \"cost =\", \"{:.5f}\".format(avg_cost) )\n",
    "    \n",
    "    print( \"\\nTraining complete!\" )\n",
    "    \n",
    "    \n",
    "    # find predictions on val set\n",
    "    pred_temp = tf.equal(tf.argmax(output_layer, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(pred_temp, \"float\"))\n",
    "    print( \"Validation Accuracy:\", accuracy.eval({x: val_x.reshape(-1, input_num_units), y: dense_to_one_hot(val_y)}) )\n",
    "    \n",
    "    predict = tf.argmax(output_layer, 1)\n",
    "    pred = predict.eval({x: test_x.reshape(-1, input_num_units)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"out.txt\", pred, fmt=\"%d\",)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
