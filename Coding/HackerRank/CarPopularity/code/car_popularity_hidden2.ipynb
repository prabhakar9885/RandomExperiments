{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.misc import imread\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 128\n",
    "rng = np.random.RandomState(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_dir = os.path.abspath('..')\n",
    "data_dir = os.path.join(root_dir, 'data')\n",
    "\n",
    "# check for existence\n",
    "os.path.exists(root_dir)\n",
    "os.path.exists(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(os.path.join(data_dir, 'train.csv'), dtype=np.float32)\n",
    "test = pd.read_csv(os.path.join(data_dir, 'test.csv'), header=None, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train.iloc[:, :-1].values\n",
    "test_x = test.iloc[:, :].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_size = int(train_x.shape[0]*0.7)\n",
    "\n",
    "train_x, val_x = train_x[:split_size], train_x[split_size:]\n",
    "train_y, val_y = train.popularity.values[:split_size], train.popularity.values[split_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auxilary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_to_one_hot(labels_dense, num_classes=4):\n",
    "    \"\"\"Convert class labels from scalars to one-hot vectors\"\"\"\n",
    "    s = pd.Series(list(range(1,num_classes+1,1)))\n",
    "    labels_one_hot = pd.get_dummies(labels_dense)\n",
    "    \n",
    "#     num_labels = labels_dense.shape[0]\n",
    "#     index_offset = np.arange(num_labels) * num_classes\n",
    "#     labels_one_hot = np.zeros((num_labels, num_classes))\n",
    "#     labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    "    \n",
    "    return labels_one_hot\n",
    "\n",
    "def preproc(unclean_batch_x):\n",
    "    \"\"\"Convert values to range 0-1, with mean centered\"\"\"\n",
    "    temp_batch = (unclean_batch_x-unclean_batch_x.min()) / (unclean_batch_x.max()-unclean_batch_x.min())\n",
    "    \n",
    "    return temp_batch\n",
    "\n",
    "def batch_creator(batch_size, dataset_length, dataset_name):\n",
    "    \"\"\"Create batch with random samples and return appropriate format\"\"\"\n",
    "    batch_mask = rng.choice(dataset_length, batch_size)\n",
    "    \n",
    "    batch_x = eval(dataset_name + '_x')[[batch_mask]].reshape(-1, input_num_units)\n",
    "    batch_x = preproc(batch_x)\n",
    "    \n",
    "    if dataset_name == 'train':\n",
    "        batch_y = eval(dataset_name).ix[batch_mask, 'popularity'].values\n",
    "        batch_y = dense_to_one_hot(batch_y)\n",
    "        \n",
    "    return batch_x, batch_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "### set all variables\n",
    "\n",
    "# number of neurons in each layer\n",
    "input_num_units = 6\n",
    "hidden_num_units_1 = 100\n",
    "hidden_num_units_2 = 100\n",
    "output_num_units = 4\n",
    "\n",
    "# define placeholders\n",
    "x = tf.placeholder(tf.float32, [None, input_num_units])\n",
    "y = tf.placeholder(tf.float32, [None, output_num_units])\n",
    "\n",
    "# set remaining variables\n",
    "epochs = 250\n",
    "batch_size = 512\n",
    "learning_rate = 0.01\n",
    "\n",
    "### define weights and biases of the neural network (refer this article if you don't understand the terminologies)\n",
    "\n",
    "weights = {\n",
    "    'hidden_1': tf.Variable(tf.random_normal([input_num_units, hidden_num_units_1], seed=seed)),\n",
    "    'hidden_2': tf.Variable(tf.random_normal([hidden_num_units_1, hidden_num_units_2], seed=seed)),\n",
    "    'output': tf.Variable(tf.random_normal([hidden_num_units_2, output_num_units], seed=seed))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'hidden_1': tf.Variable(tf.random_normal([hidden_num_units_1], seed=seed)),\n",
    "    'hidden_2': tf.Variable(tf.random_normal([hidden_num_units_2], seed=seed)),\n",
    "    'output': tf.Variable(tf.random_normal([output_num_units], seed=seed))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computational Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_1 = tf.add(tf.matmul(x, weights['hidden_1']), biases['hidden_1'])\n",
    "hidden_layer_1 = tf.nn.relu(hidden_layer_1)\n",
    "hidden_layer_2 = tf.add(tf.matmul(hidden_layer_1, weights['hidden_2']), biases['hidden_2'])\n",
    "hidden_layer_2 = tf.nn.relu(hidden_layer_2)\n",
    "\n",
    "output_layer = tf.matmul(hidden_layer_2, weights['output']) + biases['output']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=output_layer))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 cost = 15.98574\n",
      "Epoch: 2 cost = 9.91691\n",
      "Epoch: 3 cost = 7.93598\n",
      "Epoch: 4 cost = 5.46604\n",
      "Epoch: 5 cost = 4.48811\n",
      "Epoch: 6 cost = 3.53503\n",
      "Epoch: 7 cost = 2.85252\n",
      "Epoch: 8 cost = 2.12825\n",
      "Epoch: 9 cost = 2.40547\n",
      "Epoch: 10 cost = 2.33663\n",
      "Epoch: 11 cost = 1.50669\n",
      "Epoch: 12 cost = 1.12900\n",
      "Epoch: 13 cost = 1.16287\n",
      "Epoch: 14 cost = 0.88149\n",
      "Epoch: 15 cost = 0.86997\n",
      "Epoch: 16 cost = 0.75072\n",
      "Epoch: 17 cost = 0.61069\n",
      "Epoch: 18 cost = 0.51549\n",
      "Epoch: 19 cost = 0.43406\n",
      "Epoch: 20 cost = 0.43250\n",
      "Epoch: 21 cost = 0.34173\n",
      "Epoch: 22 cost = 0.35016\n",
      "Epoch: 23 cost = 0.35800\n",
      "Epoch: 24 cost = 0.35546\n",
      "Epoch: 25 cost = 0.29966\n",
      "Epoch: 26 cost = 0.28586\n",
      "Epoch: 27 cost = 0.27902\n",
      "Epoch: 28 cost = 0.25076\n",
      "Epoch: 29 cost = 0.23185\n",
      "Epoch: 30 cost = 0.20206\n",
      "Epoch: 31 cost = 0.21123\n",
      "Epoch: 32 cost = 0.17013\n",
      "Epoch: 33 cost = 0.17481\n",
      "Epoch: 34 cost = 0.19364\n",
      "Epoch: 35 cost = 0.21177\n",
      "Epoch: 36 cost = 0.20383\n",
      "Epoch: 37 cost = 0.16818\n",
      "Epoch: 38 cost = 0.17998\n",
      "Epoch: 39 cost = 0.13636\n",
      "Epoch: 40 cost = 0.17146\n",
      "Epoch: 41 cost = 0.16320\n",
      "Epoch: 42 cost = 0.16637\n",
      "Epoch: 43 cost = 0.12086\n",
      "Epoch: 44 cost = 0.12516\n",
      "Epoch: 45 cost = 0.12775\n",
      "Epoch: 46 cost = 0.17172\n",
      "Epoch: 47 cost = 0.15204\n",
      "Epoch: 48 cost = 0.13872\n",
      "Epoch: 49 cost = 0.12095\n",
      "Epoch: 50 cost = 0.11923\n",
      "Epoch: 51 cost = 0.10957\n",
      "Epoch: 52 cost = 0.10980\n",
      "Epoch: 53 cost = 0.13406\n",
      "Epoch: 54 cost = 0.10352\n",
      "Epoch: 55 cost = 0.09405\n",
      "Epoch: 56 cost = 0.13567\n",
      "Epoch: 57 cost = 0.11449\n",
      "Epoch: 58 cost = 0.10718\n",
      "Epoch: 59 cost = 0.08515\n",
      "Epoch: 60 cost = 0.08138\n",
      "Epoch: 61 cost = 0.09946\n",
      "Epoch: 62 cost = 0.11030\n",
      "Epoch: 63 cost = 0.08042\n",
      "Epoch: 64 cost = 0.11058\n",
      "Epoch: 65 cost = 0.11407\n",
      "Epoch: 66 cost = 0.10772\n",
      "Epoch: 67 cost = 0.12478\n",
      "Epoch: 68 cost = 0.10793\n",
      "Epoch: 69 cost = 0.07334\n",
      "Epoch: 70 cost = 0.06346\n",
      "Epoch: 71 cost = 0.10290\n",
      "Epoch: 72 cost = 0.10364\n",
      "Epoch: 73 cost = 0.10970\n",
      "Epoch: 74 cost = 0.10710\n",
      "Epoch: 75 cost = 0.13496\n",
      "Epoch: 76 cost = 0.12940\n",
      "Epoch: 77 cost = 0.10132\n",
      "Epoch: 78 cost = 0.08423\n",
      "Epoch: 79 cost = 0.10652\n",
      "Epoch: 80 cost = 0.11171\n",
      "Epoch: 81 cost = 0.11262\n",
      "Epoch: 82 cost = 0.07383\n",
      "Epoch: 83 cost = 0.07879\n",
      "Epoch: 84 cost = 0.07475\n",
      "Epoch: 85 cost = 0.10607\n",
      "Epoch: 86 cost = 0.08972\n",
      "Epoch: 87 cost = 0.07450\n",
      "Epoch: 88 cost = 0.06863\n",
      "Epoch: 89 cost = 0.05427\n",
      "Epoch: 90 cost = 0.06932\n",
      "Epoch: 91 cost = 0.06633\n",
      "Epoch: 92 cost = 0.06242\n",
      "Epoch: 93 cost = 0.05014\n",
      "Epoch: 94 cost = 0.06257\n",
      "Epoch: 95 cost = 0.05902\n",
      "Epoch: 96 cost = 0.06656\n",
      "Epoch: 97 cost = 0.04725\n",
      "Epoch: 98 cost = 0.06611\n",
      "Epoch: 99 cost = 0.05315\n",
      "Epoch: 100 cost = 0.05497\n",
      "Epoch: 101 cost = 0.05722\n",
      "Epoch: 102 cost = 0.06717\n",
      "Epoch: 103 cost = 0.04869\n",
      "Epoch: 104 cost = 0.05393\n",
      "Epoch: 105 cost = 0.07062\n",
      "Epoch: 106 cost = 0.06554\n",
      "Epoch: 107 cost = 0.06292\n",
      "Epoch: 108 cost = 0.06100\n",
      "Epoch: 109 cost = 0.04499\n",
      "Epoch: 110 cost = 0.04025\n",
      "Epoch: 111 cost = 0.04415\n",
      "Epoch: 112 cost = 0.04505\n",
      "Epoch: 113 cost = 0.05250\n",
      "Epoch: 114 cost = 0.04583\n",
      "Epoch: 115 cost = 0.05204\n",
      "Epoch: 116 cost = 0.05496\n",
      "Epoch: 117 cost = 0.05598\n",
      "Epoch: 118 cost = 0.05916\n",
      "Epoch: 119 cost = 0.05957\n",
      "Epoch: 120 cost = 0.03931\n",
      "Epoch: 121 cost = 0.04192\n",
      "Epoch: 122 cost = 0.04339\n",
      "Epoch: 123 cost = 0.05103\n",
      "Epoch: 124 cost = 0.05109\n",
      "Epoch: 125 cost = 0.06557\n",
      "Epoch: 126 cost = 0.05176\n",
      "Epoch: 127 cost = 0.05490\n",
      "Epoch: 128 cost = 0.06945\n",
      "Epoch: 129 cost = 0.05533\n",
      "Epoch: 130 cost = 0.06352\n",
      "Epoch: 131 cost = 0.08531\n",
      "Epoch: 132 cost = 0.06166\n",
      "Epoch: 133 cost = 0.11449\n",
      "Epoch: 134 cost = 0.05135\n",
      "Epoch: 135 cost = 0.06529\n",
      "Epoch: 136 cost = 0.08139\n",
      "Epoch: 137 cost = 0.09275\n",
      "Epoch: 138 cost = 0.09088\n",
      "Epoch: 139 cost = 0.10091\n",
      "Epoch: 140 cost = 0.10495\n",
      "Epoch: 141 cost = 0.08122\n",
      "Epoch: 142 cost = 0.05817\n",
      "Epoch: 143 cost = 0.07306\n",
      "Epoch: 144 cost = 0.07318\n",
      "Epoch: 145 cost = 0.06432\n",
      "Epoch: 146 cost = 0.07588\n",
      "Epoch: 147 cost = 0.04518\n",
      "Epoch: 148 cost = 0.03811\n",
      "Epoch: 149 cost = 0.05726\n",
      "Epoch: 150 cost = 0.05819\n",
      "Epoch: 151 cost = 0.04828\n",
      "Epoch: 152 cost = 0.03212\n",
      "Epoch: 153 cost = 0.04431\n",
      "Epoch: 154 cost = 0.04276\n",
      "Epoch: 155 cost = 0.04560\n",
      "Epoch: 156 cost = 0.05518\n",
      "Epoch: 157 cost = 0.06029\n",
      "Epoch: 158 cost = 0.04692\n",
      "Epoch: 159 cost = 0.04701\n",
      "Epoch: 160 cost = 0.04960\n",
      "Epoch: 161 cost = 0.04044\n",
      "Epoch: 162 cost = 0.04082\n",
      "Epoch: 163 cost = 0.05224\n",
      "Epoch: 164 cost = 0.05064\n",
      "Epoch: 165 cost = 0.04426\n",
      "Epoch: 166 cost = 0.03193\n",
      "Epoch: 167 cost = 0.03959\n",
      "Epoch: 168 cost = 0.04127\n",
      "Epoch: 169 cost = 0.04659\n",
      "Epoch: 170 cost = 0.03145\n",
      "Epoch: 171 cost = 0.03162\n",
      "Epoch: 172 cost = 0.03085\n",
      "Epoch: 173 cost = 0.03538\n",
      "Epoch: 174 cost = 0.03515\n",
      "Epoch: 175 cost = 0.02945\n",
      "Epoch: 176 cost = 0.02857\n",
      "Epoch: 177 cost = 0.03848\n",
      "Epoch: 178 cost = 0.04275\n",
      "Epoch: 179 cost = 0.03920\n",
      "Epoch: 180 cost = 0.04031\n",
      "Epoch: 181 cost = 0.04572\n",
      "Epoch: 182 cost = 0.04025\n",
      "Epoch: 183 cost = 0.03490\n",
      "Epoch: 184 cost = 0.03181\n",
      "Epoch: 185 cost = 0.03558\n",
      "Epoch: 186 cost = 0.03092\n",
      "Epoch: 187 cost = 0.04334\n",
      "Epoch: 188 cost = 0.03788\n",
      "Epoch: 189 cost = 0.02878\n",
      "Epoch: 190 cost = 0.03243\n",
      "Epoch: 191 cost = 0.03905\n",
      "Epoch: 192 cost = 0.02286\n",
      "Epoch: 193 cost = 0.02680\n",
      "Epoch: 194 cost = 0.02382\n",
      "Epoch: 195 cost = 0.03468\n",
      "Epoch: 196 cost = 0.02138\n",
      "Epoch: 197 cost = 0.04175\n",
      "Epoch: 198 cost = 0.02399\n",
      "Epoch: 199 cost = 0.03039\n",
      "Epoch: 200 cost = 0.02886\n",
      "Epoch: 201 cost = 0.02135\n",
      "Epoch: 202 cost = 0.03067\n",
      "Epoch: 203 cost = 0.03219\n",
      "Epoch: 204 cost = 0.03668\n",
      "Epoch: 205 cost = 0.05748\n",
      "Epoch: 206 cost = 0.04885\n",
      "Epoch: 207 cost = 0.07865\n",
      "Epoch: 208 cost = 0.07080\n",
      "Epoch: 209 cost = 0.05081\n",
      "Epoch: 210 cost = 0.05920\n",
      "Epoch: 211 cost = 0.04111\n",
      "Epoch: 212 cost = 0.04244\n",
      "Epoch: 213 cost = 0.06834\n",
      "Epoch: 214 cost = 0.04358\n",
      "Epoch: 215 cost = 0.06198\n",
      "Epoch: 216 cost = 0.05830\n",
      "Epoch: 217 cost = 0.05829\n",
      "Epoch: 218 cost = 0.06867\n",
      "Epoch: 219 cost = 0.05820\n",
      "Epoch: 220 cost = 0.03924\n",
      "Epoch: 221 cost = 0.03397\n",
      "Epoch: 222 cost = 0.04610\n",
      "Epoch: 223 cost = 0.03294\n",
      "Epoch: 224 cost = 0.03374\n",
      "Epoch: 225 cost = 0.02864\n",
      "Epoch: 226 cost = 0.02749\n",
      "Epoch: 227 cost = 0.02558\n",
      "Epoch: 228 cost = 0.03562\n",
      "Epoch: 229 cost = 0.03022\n",
      "Epoch: 230 cost = 0.02744\n",
      "Epoch: 231 cost = 0.02290\n",
      "Epoch: 232 cost = 0.02778\n",
      "Epoch: 233 cost = 0.02418\n",
      "Epoch: 234 cost = 0.03100\n",
      "Epoch: 235 cost = 0.02380\n",
      "Epoch: 236 cost = 0.02578\n",
      "Epoch: 237 cost = 0.02472\n",
      "Epoch: 238 cost = 0.02316\n",
      "Epoch: 239 cost = 0.01987\n",
      "Epoch: 240 cost = 0.01851\n",
      "Epoch: 241 cost = 0.02591\n",
      "Epoch: 242 cost = 0.03141\n",
      "Epoch: 243 cost = 0.02970\n",
      "Epoch: 244 cost = 0.02740\n",
      "Epoch: 245 cost = 0.02813\n",
      "Epoch: 246 cost = 0.03887\n",
      "Epoch: 247 cost = 0.03880\n",
      "Epoch: 248 cost = 0.02910\n",
      "Epoch: 249 cost = 0.04060\n",
      "Epoch: 250 cost = 0.02649\n",
      "\n",
      "Training complete!\n",
      "Validation Accuracy: 0.60940695\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        avg_cost = 0\n",
    "        total_batch = int(train.shape[0]/batch_size)\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = batch_creator(batch_size, train_x.shape[0], 'train')\n",
    "            _, c = sess.run([optimizer, cost], feed_dict = {x: batch_x, y: batch_y})\n",
    "            \n",
    "            avg_cost += c / total_batch\n",
    "            \n",
    "        print( \"Epoch:\", (epoch+1), \"cost =\", \"{:.5f}\".format(avg_cost) )\n",
    "    \n",
    "    print( \"\\nTraining complete!\" )\n",
    "    \n",
    "    \n",
    "    # find predictions on val set\n",
    "    pred_temp = tf.equal(tf.argmax(output_layer, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(pred_temp, \"float\"))\n",
    "    print( \"Validation Accuracy:\", accuracy.eval({x: val_x.reshape(-1, input_num_units), y: dense_to_one_hot(val_y)}) )\n",
    "    \n",
    "    predict = tf.argmax(output_layer, 1)\n",
    "    pred = predict.eval({x: test_x.reshape(-1, input_num_units)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"out.txt\", pred, fmt=\"%d\",)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
